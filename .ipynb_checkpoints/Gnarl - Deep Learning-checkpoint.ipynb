{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gnarl - An Easy to use Deep Learning Framework for Python\n",
    "\n",
    "Welcome to Gnarl, a lightweight deep learning network built in Python using Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "\n",
    "np.seterr(all='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions to help with implementations and checks of Node classes\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is an `Input` node and \n",
    "                the value is the respective value feed to that node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: The output node of the graph (no outgoing edges).\n",
    "        `sorted_nodes`: a topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output node's value\n",
    "    \"\"\"\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"Performs a forward pass and a backward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-4):\n",
    "    \"\"\"Updates the value of each trainable with SGD.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `trainables`: A list of `Input` Nodes representing weights/biases.\n",
    "        `learning_rate`: The learning rate.\n",
    "    \"\"\"\n",
    "    for t in trainables:\n",
    "        partial = t.gradients[t]\n",
    "        t.value -= learning_rate * partial    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a computational graph using Nodes and Edges\n",
    "\n",
    "First, we define a generic Node class object that takes a list of inbound nodes as its input argument, initializes itself and attaches itself as part of the list of outbound nodes to each inbound node.\n",
    "\n",
    "### Node(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"A node in a computational graph.\"\"\"\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        self.outbound_nodes = []\n",
    "        self.value = None # Init first value as None\n",
    "        self.gradients = {}\n",
    "        \n",
    "        # Append this node to all nodes that point to it\n",
    "        for in_node in self.inbound_nodes:\n",
    "            in_node.outbound_nodes.append(self)\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"Forward propagate input from inbound nodes to outbound nodes.\"\"\"\n",
    "        raise NotImplemented\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backward propagate output from outbound nodes to inbound nodes.\"\"\"\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Subclasses to Node\n",
    "We will need to subclass the Node base class so that we have a set of nodes that are able to perform different types of computation required to build a fully connected computational graph.\n",
    "\n",
    "### Input(Node) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        \"\"\"An input node. Input nodes don't perform any computations.\n",
    "        Rather, they represent the input features that will be fed into \n",
    "        the neural network.\n",
    "        \"\"\"\n",
    "        Node.__init__(self)\n",
    "    \n",
    "    def forward(self, value=None):\n",
    "        \"\"\"Forward propagate input value\"\"\"\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backward propagate from outbound nodes to this node.\"\"\"\n",
    "        self.gradients = {self: 0}\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] += grad_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add(Node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"A node that adds its input nodes together.\"\"\"\n",
    "        Node.__init__(self, [*args])\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"Adds the values of the input nodes and sets the value\n",
    "        of the node.\n",
    "        \"\"\"\n",
    "        self.value = 0.\n",
    "        for in_node in self.inbound_nodes:\n",
    "            try:\n",
    "                self.value += in_node.value\n",
    "            except:\n",
    "                print('Couldn\\'t add value from input node:', in_node.value)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backward propagate from outbound nodes to this node.\"\"\"\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        for idx, n in enumerate(self.outbound_nodes):\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self.inbound_nodes[idx]] += grad_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract(Node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Subtract(Node):\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"A node that subtracts inputs.\"\"\"\n",
    "        Node.__init__(self, [x, y])\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"Compute forward propagation of node value.\"\"\"\n",
    "        self.value = self.inbound_nodes[0].value\n",
    "        for in_node in self.inbound_nodes[1:]:\n",
    "            try:\n",
    "                self.value -= in_node.value\n",
    "            except:\n",
    "                print('Couldn\\'t subract value from input node:', in_node.value)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Compute backward propagation of node.\"\"\"\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        for idx, n in enumerate(self.outbound_nodes):\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self.inbound_nodes[idx]] -= grad_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mul(Node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Mul(Node):\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"A node that multiplies its input nodes together.\"\"\"\n",
    "        Node.__init__(self, [x, y])\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"Multiplies the value of the input nodes and sets the value\n",
    "        of the node.\"\"\"\n",
    "        x = self.inbound_nodes[0].value\n",
    "        y = self.inbound_nodes[1].value\n",
    "        \n",
    "        try:\n",
    "            self.value = x.dot(y)\n",
    "        except:\n",
    "            print('Couldn\\'t mutiply the value from input node:', in_node.value)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        for idx, n in enumerate(self.outbound_nodes):\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self.inbound_nodes[idx]] *= grad_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149.0\n",
      "[ 0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "ns = [Input() for n in range(0,5)]\n",
    "i = np.array([1., 2., 3.])\n",
    "for n in ns:\n",
    "    n.forward(i)\n",
    "    i += 1\n",
    "m = Mul(ns[1], ns[2])\n",
    "m.forward()\n",
    "print(m.value)\n",
    "m.backward()\n",
    "print(m.gradients[ns[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide(Node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Divide(Node):\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"Compute the element-wise division between two input nodes.\"\"\"\n",
    "        Node.__init__(self, [x, y])\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"Forward propagate the value from dividing one input node with\n",
    "        the other.\"\"\"\n",
    "        self.x = self.inbound_nodes[0].value\n",
    "        self.y = self.inbound_nodes[1].value\n",
    "        \n",
    "        self.value = np.divide(self.x, self.y) \n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backward propagate value from outbound node gradients.\"\"\"\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        \n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self.inbound_nodes[0]] = 1. / self.y * grad_cost\n",
    "            self.gradients[self.inbound_nodes[1]] = - self.x / (self.y**2) * grad_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5         1.        ]\n",
      " [ 0.33333333  1.        ]]\n",
      "{<__main__.Input object at 0x10a552d30>: array([[0, 0],\n",
      "       [0, 0]]), <__main__.Input object at 0x10a552da0>: array([[0, 0],\n",
      "       [0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "x, y = Input(), Input()\n",
    "\n",
    "x.forward(value=np.array([[1,2], [3,4]]))\n",
    "y.forward(value=np.array([[2,2], [9, 4]]))\n",
    "\n",
    "d = Divide(x, y)\n",
    "d.forward()\n",
    "print(d.value)\n",
    "d.backward()\n",
    "print(d.gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear(Node)\n",
    "Represents a linear combination of input nodes and their respective weights inside a hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"A node that computes the linear combination of a list of input\n",
    "        nodes features, a list of input weights, and a bias term.\"\"\"\n",
    "        Node.__init__(self, [*args])\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"Compute the linear combination of the inbound nodes.\"\"\"\n",
    "        \n",
    "        # Create vectors\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        \n",
    "        if self.inbound_nodes[2] is None:\n",
    "            # No bias term\n",
    "            self.value = np.dot(X, W)\n",
    "        else:\n",
    "            b = self.inbound_nodes[2].value\n",
    "            self.value = np.dot(X, W) + b\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Compute the backward propagation of the node.\"\"\"\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        for n in self.outbound_nodes:\n",
    "            # Find current grad\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            # Compute partial grads\n",
    "            dX = np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            dW = np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            cumul_b = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "            \n",
    "            # Add to current node grads\n",
    "            self.gradients[self.inbound_nodes[0]] += dX\n",
    "            self.gradients[self.inbound_nodes[1]] += dW\n",
    "            self.gradients[self.inbound_nodes[2]] += cumul_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.7\n"
     ]
    }
   ],
   "source": [
    "inputs, weights, bias = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(inputs, weights, bias)\n",
    "\n",
    "feed_dict = {\n",
    "    inputs: [6, 14, 3],\n",
    "    weights: [0.5, 0.25, 1.4],\n",
    "    bias: 2\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(output) # should be 12.7 with this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.  4.]\n",
      " [-9.  4.]]\n"
     ]
    }
   ],
   "source": [
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[-9., 4.],\n",
    "[-9., 4.]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Nodes\n",
    "The following are some activation functions represented as nodes in the computational graph that a neural network can be represented as.\n",
    "\n",
    "### Sigmoid(Node)\n",
    "Mainly used in logistic regression and simplistic neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        \"\"\"Compute the sigmoid of a given input node.\"\"\"\n",
    "        Node.__init__(self, [node])\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute the sigmoid for an input z.\"\"\"\n",
    "        return 1./(1 + np.exp(-z))\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"Compute the value of the Sigmoid node.\"\"\"\n",
    "        Z = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(Z)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        \n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            dZ = sigmoid * (1. - sigmoid)\n",
    "            self.gradients[self.inbound_nodes[0]] += dZ * grad_cost # Ele wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.23394576e-04   9.82013790e-01]\n",
      " [  1.23394576e-04   9.82013790e-01]]\n"
     ]
    }
   ],
   "source": [
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "g = Sigmoid(f)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(g, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[  1.23394576e-04   9.82013790e-01]\n",
    " [  1.23394576e-04   9.82013790e-01]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU(Node)\n",
    "More stable than the sigmoid. Runs risk of dead neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Node):\n",
    "    def __init__(self, node, epsilon=1e-4):\n",
    "        \"\"\"Computes rectified linear units for the node.\"\"\"\n",
    "        Node.__init__(self, [node])\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"Forward propagate node values.\"\"\"\n",
    "        eps = np.zeros_like(self.inbound_nodes[0].value) + self.epsilon\n",
    "        self.value = np.maximum(eps, self.inbound_nodes[0].value)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backward propagate node gradients\"\"\"\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        \n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            grad_cost[self.value <= self.epsilon] = 0. # Kill gradients where value is 0.\n",
    "            self.gradients[self.inbound_nodes[0]] += grad_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e-04   1.00000000e+00]\n",
      " [  2.00000000e+00   1.00000000e-04]]\n"
     ]
    }
   ],
   "source": [
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "g = ReLU(f)\n",
    "\n",
    "X_ = np.array([[-1., 2.], \n",
    "               [1, -1]])\n",
    "W_ = np.array([[2., 1.], \n",
    "               [0., 1.]])\n",
    "b_ = np.array([0., 0.])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(g, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[  0   1]\n",
    " [  2   0]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyReLU(Node)\n",
    "ReLU to avoid dead neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(Node):\n",
    "    def __init__(self, node, epsilon=0., leak=1e-2):\n",
    "        \"\"\"Computes leaky rectified linear units for the node.\"\"\"\n",
    "        Node.__init__(self, [node])\n",
    "        self.epsilon = epsilon\n",
    "        self.leak = leak\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"Forward propagate node values.\"\"\"\n",
    "        #eps = np.zeros_like(self.inbound_nodes[0].value) + self.epsilon\n",
    "        \n",
    "        #print('Forward bef:', self.inbound_nodes[0].value)\n",
    "        self.value = np.maximum(self.epsilon, self.inbound_nodes[0].value)\n",
    "        #print('Forward aft:', self.value)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backward propagate node gradients\"\"\"\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        \n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            grad_cost[self.value <= self.epsilon] = self.leak\n",
    "            \n",
    "            self.gradients[self.inbound_nodes[0]] += grad_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function (Output) Nodes\n",
    "The following are some examples of commonly used cost functions used to compute the residuals or errors between actual outputs and estimated outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE(Node)\n",
    "Works best for continuous outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y, y_hat):\n",
    "        \"\"\"A node that computes the mean squared error. \n",
    "        Should only be used at the last node in a network.\"\"\"\n",
    "        Node.__init__(self, [y, y_hat])\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"Compute the mean squared errror value for the node.\"\"\"\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        y_hat = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        \n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        self.error = y - y_hat\n",
    "        \n",
    "        self.value = np.mean(np.square(self.error))\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Compute the gradient for the MSE.\"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2. / self.m) * self.error\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2. / self.m) * self.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4166666667\n"
     ]
    }
   ],
   "source": [
    "y, y_hat = Input(), Input()\n",
    "cost = MSE(y, y_hat)\n",
    "\n",
    "y_ = np.array([1, 2, 3])\n",
    "y_hat_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y: y_, y_hat: y_hat_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "forward_and_backward(graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "23.4166666667\n",
    "\"\"\"\n",
    "print(cost.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogLoss(Node)\n",
    "Should only be used for probabilistic output labels between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogLoss(Node):\n",
    "    def __init__(self, y, y_hat):\n",
    "        \"\"\"A node that represents the l2 cost function.\n",
    "        1/m * sum [ (y - 1)*log(1 - y_hat) - y * log(y_hat) ].\n",
    "        Should always be last node in a graph, and only useful for probabilistic\n",
    "        outputs between 0 and 1.\"\"\"\n",
    "        Node.__init__(self, [y, y_hat])\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"Forward propagate node value, e.g. the loss/cost\"\"\"\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        y_hat = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        \n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        self.error = (y - 1.) * np.log(1. - y_hat) - y * np.log(y_hat)\n",
    "        \n",
    "        self.value = np.mean(self.error)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backward propagate gradient weights.\"\"\"\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        y_hat = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        \n",
    "        self.gradients[self.inbound_nodes[0]] = (1. / self.m) * (np.log(1. - y_hat) \\\n",
    "                                                 - np.log(y_hat))\n",
    "        self.gradients[self.inbound_nodes[1]] = (1. / self.m) * ((y - 1.) * 1. / (1. - y_hat) * -1. \\\n",
    "                                                - y / (y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0452045196096\n"
     ]
    }
   ],
   "source": [
    "y, y_hat = Input(), Input()\n",
    "cost = LogLoss(y, y_hat)\n",
    "\n",
    "y_ = np.array([1, 0, 1])\n",
    "y_hat_ = np.array([0.99, 0.1, 0.98])\n",
    "\n",
    "feed_dict = {y: y_, y_hat: y_hat_}\n",
    "graph = topological_sort(feed_dict)\n",
    "\n",
    "# forward pass\n",
    "forward_and_backward(graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "0.0452045196096\n",
    "\"\"\"\n",
    "print(cost.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax(Node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Softmax(Node):\n",
    "    def __init__(self, y, y_hat):\n",
    "        \"\"\"A node that represents the softmax loss. \n",
    "        Should always be last node in a computational graph. \n",
    "        Only useful for multinomial classification problems.\n",
    "        \"\"\"\n",
    "        Node.__init__(self, [y, y_hat])\n",
    "    \n",
    "    def _softmax(self, z):\n",
    "        z -= np.max(z) # To prevent blowup\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"Forward propagate the node value.\"\"\"\n",
    "        y = self.inbound_nodes[0].value\n",
    "        y_hat = self.inbound_nodes[1].value\n",
    "        \n",
    "        probs = self._softmax(y_hat)\n",
    "        self.probs = probs # For backward prop  \n",
    "        y_log_probs = - np.log(probs[range(len(y)), y])\n",
    "        \n",
    "        loss = np.sum(y_log_probs) / len(y)      \n",
    "        self.value = loss\n",
    "        \n",
    "        # TODO: Add regularization\n",
    "        \n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagate gradients.\"\"\"\n",
    "        grad_probs = self.probs\n",
    "        grad_probs[range(len(self.inbound_nodes[0].value)), \n",
    "                         self.inbound_nodes[0].value] -= 1\n",
    "        grad_probs /= len(self.inbound_nodes[0].value)\n",
    "        \n",
    "        #self.gradients[self.inbound_nodes[0]] = \n",
    "        #self.gradients[self.inbound_nodes[1]] = grad_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.43920315  0.29440668  0.26639018]\n",
      " [ 0.25462853  0.46396343  0.28140804]\n",
      " [ 0.21438726  0.21438726  0.57122548]]\n",
      "3.66404736725\n",
      "3.66404736725 {}\n"
     ]
    }
   ],
   "source": [
    "y, y_hat = Input(), Input()\n",
    "cost = Softmax(y, y_hat)\n",
    "\n",
    "y_ = np.array([[1,0,0], [0,1,0], [0,0,1]])\n",
    "y_hat_ = np.array([[0.7,0.3,0.2], [0.10,0.7,0.2], [0,0,0.98]])\n",
    "\n",
    "feed_dict = {y: y_, y_hat: y_hat_}\n",
    "graph = topological_sort(feed_dict)\n",
    "\n",
    "p = np.exp(y_hat_) / np.sum(np.exp(y_hat_), axis=1, keepdims=True)\n",
    "y_log_probs = - np.log(p[range(len(y_)), y_])\n",
    "loss = np.sum(y_log_probs)/len(y_)\n",
    "\n",
    "print(p)\n",
    "print(loss)\n",
    "\n",
    "# forward pass\n",
    "#forward_and_backward(graph)\n",
    "forward_pass(cost, graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(cost.value, cost.gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 247.226\n",
      "Epoch: 100, Loss: 13.693"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test the nn architecture.\n",
    "\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden_l1 = 10\n",
    "n_hidden_l2 = 5\n",
    "n_output = 1\n",
    "\n",
    "# First hidden layer\n",
    "W1_ = np.random.randn(n_features, n_hidden_l1)\n",
    "b1_ = np.zeros(n_hidden_l1)\n",
    "\n",
    "# Second hidden layer\n",
    "W2_ = np.random.randn(n_hidden_l1, n_hidden_l2)\n",
    "b2_ = np.zeros(n_hidden_l2)\n",
    "\n",
    "# Output layer\n",
    "W3_ = np.random.randn(n_hidden_l2, n_output)\n",
    "b3_ = np.zeros(n_output)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "W3, b3 = Input(), Input()\n",
    "\n",
    "# First hidden node\n",
    "l1 = Linear(X, W1, b1)\n",
    "#s1 = Sigmoid(l1)\n",
    "s1 = LeakyReLU(l1, leak=0.)\n",
    "\n",
    "# Second hidden node\n",
    "l2 = Linear(s1, W2, b2)\n",
    "#s2 = Sigmoid(l2)\n",
    "s2 = LeakyReLU(l2, leak=0.)\n",
    "\n",
    "# Output node\n",
    "l3 = Linear(s2, W3, b3)\n",
    "\n",
    "cost = MSE(y, l3)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_,\n",
    "    W3: W3_,\n",
    "    b3: b3_\n",
    "}\n",
    "\n",
    "epochs = 100\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2, W3, b3]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        sgd_update(trainables, learning_rate=1e-4)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    if i != 1: # Weird bugs otherwise\n",
    "        sys.stdout.write(\"\\rEpoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "    if i % 500 == 0:\n",
    "        print('') # Force new epoch printline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Input at 0x10f24c630>,\n",
       " <__main__.Input at 0x10a5526d8>,\n",
       " <__main__.Input at 0x10f24cf28>,\n",
       " <__main__.Input at 0x10f24c940>,\n",
       " <__main__.Input at 0x10f24cf60>,\n",
       " <__main__.Input at 0x10f24cf98>,\n",
       " <__main__.Input at 0x10a5527b8>,\n",
       " <__main__.Input at 0x10f24cfd0>,\n",
       " <__main__.Linear at 0x10f24c5f8>,\n",
       " <__main__.LeakyReLU at 0x10f24c550>,\n",
       " <__main__.Linear at 0x10f24c4e0>,\n",
       " <__main__.LeakyReLU at 0x10f24c588>,\n",
       " <__main__.Linear at 0x10f24c4a8>,\n",
       " <__main__.MSE at 0x10f24c9e8>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model class to represent the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn_options = {\n",
    "    'activation': 'leaky_relu',\n",
    "    'learning_rate': 1e-3,\n",
    "    'random_state': 1,\n",
    "    'shuffle': True,\n",
    "    'verbose': True,\n",
    "    'solver': 'sgd',\n",
    "    'loss': 'mse',\n",
    "    'batch_size': 11\n",
    "}\n",
    "\n",
    "# Activation functions\n",
    "ACTIVATIONS = {\n",
    "    'relu': ReLU,\n",
    "    'leaky_relu': LeakyReLU,\n",
    "    'sigmoid': Sigmoid,\n",
    "}\n",
    "\n",
    "LOSSES = {\n",
    "    'mse': MSE,\n",
    "    'logloss': LogLoss,\n",
    "    'softmax': None\n",
    "}\n",
    "\n",
    "class Gnarl(object):\n",
    "    def __init__(self, X=None, y=None, activation='leaky_relu',\n",
    "                 learning_rate=1e-4,\n",
    "                 random_state=1,\n",
    "                 shuffle=True,\n",
    "                 verbose=False,\n",
    "                 solver='sgd',\n",
    "                 loss='mse',\n",
    "                 batch_size=10):\n",
    "        \"\"\"An instance of a neural net model.\n",
    "        \n",
    "        Creates a neural network model object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Init model from options\n",
    "        self._init(X=X, y=y, activation=activation, \n",
    "                   learning_rate=learning_rate, random_state=random_state, \n",
    "                   shuffle=shuffle, verbose=verbose, solver=solver, loss=loss,\n",
    "                   batch_size=batch_size)\n",
    "    \n",
    "    def _init(self, X=None, y=None, activation='leaky_relu',\n",
    "              learning_rate=1e-4,\n",
    "              random_state=1,\n",
    "              shuffle=True,\n",
    "              verbose=False,\n",
    "              solver='sgd',\n",
    "              loss='mse',\n",
    "              batch_size=batch_size):\n",
    "        \"\"\"Initialize the model.\"\"\"\n",
    "        \n",
    "        # Todo: ensure that options are correct\n",
    "        \n",
    "        # Init dicts to hold the layers and the weights and biases\n",
    "        #self.layers = {}\n",
    "        self.nodes = {}\n",
    "        self.layers_list = []\n",
    "        self._weights = []\n",
    "        self._biases = []\n",
    "        self.trainables = []\n",
    "        \n",
    "        # Init graph\n",
    "        self.graph = []\n",
    "        \n",
    "        try:\n",
    "            self.activation = activation\n",
    "            self.learning_rate = learning_rate\n",
    "            self.random_state = random_state\n",
    "            self.shuffle = shuffle\n",
    "            self.verbose = verbose\n",
    "            self.solver = solver\n",
    "            self.loss = loss\n",
    "            self.batch_size = batch_size\n",
    "        except ValueError as e:\n",
    "            print('Init failed with error: ', e)\n",
    "        \n",
    "        # Set X and y input nodes to initiate model\n",
    "        self.X = Input()\n",
    "        self.y = Input()\n",
    "        \n",
    "        self._update_input(X)\n",
    "        self._update_output(y)\n",
    "        \n",
    "        # Set first nodes in node dict for future\n",
    "        self.nodes[self.X] = self.X.value\n",
    "        self.nodes[self.y] = self.y.value\n",
    "        \n",
    "        self._input_layer(self.X.value)\n",
    "    \n",
    "    def _update_input(self, X_train):\n",
    "        \"\"\"Update the model's feature training data.\"\"\"\n",
    "        self.X.value = X_train\n",
    "    \n",
    "    def _update_output(self, y_train):\n",
    "        \"\"\"Update the model's output training data.\"\"\"\n",
    "        self.y.value = y_train\n",
    "    \n",
    "    def _input_layer(self, X_train):\n",
    "        \"\"\"Define the model's input layer.\"\"\"\n",
    "        self.layers_list.append(self.X)\n",
    "        self._weights.append(self.nodes[self.X]) # Convenience for hidden_layer\n",
    "    \n",
    "    def _reset_graph(self):\n",
    "        \"\"\"Reset graph node values.\"\"\"\n",
    "        for node in self.trainables:\n",
    "                # If weights\n",
    "                if len(node.value.shape) == 2:\n",
    "                    node.value = np.random.randn(node.value.shape[0], node.value.shape[1])\n",
    "                # if bias\n",
    "                elif len(node.value.shape) == 1:\n",
    "                    node.value = np.zeros_like(node.value)\n",
    "    \n",
    "    def hidden_layer(self, out_nodes, \n",
    "                     activation):\n",
    "        \"\"\"Add a hidden layer to the model.\"\"\"\n",
    "        # Init random weights \n",
    "        W = Input()\n",
    "        W_ = np.random.randn(self._weights[-1].shape[1], out_nodes)\n",
    "        self._weights.append(W_) # Store weights for convenience\n",
    "        \n",
    "        # Init biases\n",
    "        b = Input()\n",
    "        b_ = np.zeros(out_nodes)\n",
    "        self._biases.append(b_) # Store biases for convenience\n",
    "        \n",
    "        # Add weights and biases to dicts for future use in connect_layers\n",
    "        self.nodes[W] = W_\n",
    "        self.nodes[b]= b_\n",
    "        \n",
    "        # Linear combo from previous layer to current layer\n",
    "        layer = Linear(self.layers_list[-1], W, b)\n",
    "        \n",
    "        # Activation\n",
    "        if activation == 'none': # Output layer is regression\n",
    "            self.layers_list.append(layer)\n",
    "        elif activation == 'softmax':\n",
    "            raise NotImplemented\n",
    "        else:\n",
    "            activation = ACTIVATIONS[activation](layer)\n",
    "            self.layers_list.append(activation)\n",
    "        \n",
    "        # Add weights and biases to trainables\n",
    "        self.trainables += [W, b]\n",
    "    \n",
    "    def connect_layers(self):\n",
    "        \"\"\"Connect and build the computational graph the network represents.\"\"\"\n",
    "        \n",
    "        # Attach loss/cost function to output layer.\n",
    "        if self.loss not in LOSSES:\n",
    "            raise NotImplemented\n",
    "        \n",
    "        if self.loss == 'softmax':\n",
    "            loss = Softmax(self.y, self.layers_list[-1])\n",
    "            #self.layers_list.append(loss)\n",
    "        elif self.loss == 'mse':\n",
    "            loss = MSE(self.y, self.layers_list[-1])\n",
    "            #self.layers_list.append(loss)\n",
    "        elif self.loss == 'logloss':\n",
    "            loss = LogLoss(self.y, self.layers_list[-1])\n",
    "            #self.layers_list.append(loss)\n",
    "        \n",
    "        self.graph = topological_sort(self.nodes)\n",
    "        \n",
    "    def fit(self, X_train, y_train, solver='sgd', epochs=10, fit_more_data=False):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        \n",
    "        # Reset graph values to purge nodes \n",
    "        if fit_more_data is False:\n",
    "            self._reset_graph()\n",
    "        \n",
    "        # Setup convenient vars\n",
    "        m = X_train.shape[0]\n",
    "        steps_per_epoch = m // self.batch_size\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Training model...')\n",
    "            print('Total number of samples:', m)\n",
    "            print('Steps per epoch:', steps_per_epoch)\n",
    "            print('='*80)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            loss = 0\n",
    "            for j in range(steps_per_epoch):\n",
    "                # Step 1\n",
    "                # Randomly sample a batch of examples\n",
    "                X_batch, y_batch = resample(X_train, y_train, n_samples=self.batch_size)\n",
    "\n",
    "                # Reset value of X and y Inputs\n",
    "                self._update_input(X_batch)\n",
    "                self._update_output(y_batch)\n",
    "\n",
    "                # Step 2\n",
    "                forward_and_backward(self.graph)\n",
    "\n",
    "                # Step 3\n",
    "                sgd_update(self.trainables, learning_rate=self.learning_rate)\n",
    "\n",
    "                loss += self.graph[-1].value\n",
    "                \n",
    "            if self.verbose:\n",
    "                sys.stdout.write(\"\\rEpoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "                if i % 500 == 0:\n",
    "                    print('') # Force new epoch printline\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Predict output using the trained model.\n",
    "        \n",
    "        Return predictions.\n",
    "        \"\"\"\n",
    "        self._update_input(X_test)\n",
    "        \n",
    "        # Forward propagate (avoid last node, as this is the loss)\n",
    "        for node in self.graph[:-1]:\n",
    "            node.forward()\n",
    "            \n",
    "        return self.graph[-2].value\n",
    "    \n",
    "    def score(self, y, y_pred, metric='accuracy'):\n",
    "        \"\"\"Compute the model's accuracy.\n",
    "        \n",
    "        Returns a float.\n",
    "        \"\"\"\n",
    "        if metric is 'accuracy':\n",
    "            try:\n",
    "                return 100. * np.mean(y == y_pred)\n",
    "            except ValueError as e:\n",
    "                print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-84e604fa6c1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Setup model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGnarl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnn_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'leaky_relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'leaky_relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-ab62d4e21510>\u001b[0m in \u001b[0;36mhidden_layer\u001b[0;34m(self, out_nodes, activation)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# Init random weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mW_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Store weights for convenience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize features\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "# Split data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.2)\n",
    "X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split(X_train, y_train, test_size=0.8)\n",
    "\n",
    "# Setup model\n",
    "net = Gnarl(X_train_1, y_train_1, **nn_options)\n",
    "net.hidden_layer(10, activation='leaky_relu')\n",
    "net.hidden_layer(5, activation='leaky_relu')\n",
    "net.hidden_layer(1, activation='none')\n",
    "net.connect_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Total number of samples: 80\n",
      "Steps per epoch: 7\n",
      "================================================================================\n",
      "Epoch: 1, Loss: 892.400\n",
      "Epoch: 500, Loss: 7.9052"
     ]
    }
   ],
   "source": [
    "net.learning_rate = 1e-4\n",
    "net.fit(X_train_1, y_train_1, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.46788218]\n"
     ]
    }
   ],
   "source": [
    "print(net.trainables[-1].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Total number of samples: 324\n",
      "Steps per epoch: 29\n",
      "================================================================================\n",
      "Epoch: 1, Loss: 36.067\n",
      "Epoch: 501, Loss: 9.2503\n",
      "Epoch: 1000, Loss: 6.530"
     ]
    }
   ],
   "source": [
    "net.fit(X_train_2, y_train_2, epochs=1000, fit_more_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.01280231]\n"
     ]
    }
   ],
   "source": [
    "print(net.trainables[-1].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.769566406144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Predict y from test data\n",
    "y_pred = net.predict(X_test)\n",
    "\n",
    "# Compute r2\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 Machine Learning",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
