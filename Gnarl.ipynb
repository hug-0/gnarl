{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gnarl - Generally nice and reasonable learning\n",
    "Welcome to Gnarl, an easy-to-use [deep learning](https://en.wikipedia.org/wiki/Deep_learning) framework for Python. Gnarl was created with the aim to help newcomers  build an understanding of how shallow and deep neural networks these machine learning methods work, what they do under the hood, and what they can be used for.\n",
    "\n",
    "## Usage\n",
    "Gnarl is designed to be as easy as possible to get started with. The following is a brief tutorial that showcases how to create deep neural networks using Gnarl.\n",
    "\n",
    "### Import Gnarl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Gnarl model\n",
    "from gnarl import Gnarl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a Gnarl model\n",
    "When instantiating Gnarl, input and output data aren't strictly required, but dummy arrays that provide the correct number of input features and output is necessary in order to build create the hidden layers behind the scenes.\n",
    "\n",
    "Let's first import some utilities and example data, and split the data set into training, validation and test sets. We are also going to normalize the input feature data, in order to avoid potential numerical instabilities and computational overflows that could otherwise occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import some utilities\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Boston housing prices from scikit-learn\n",
    "data = load_boston()\n",
    "\n",
    "# Set data input and output\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Normalize all input data to have mean 0 and standard deviation 1\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "# Split data into training, validation and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some regression data to play around with, instantiating the model is easy. You can either specify individual hyperparameters of a Gnarl model, or specify them as a dictionary object and pass this to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify hyperparameters for the model\n",
    "nn_options = {\n",
    "    'activation': 'leaky_relu',\n",
    "    'learning_rate': 1e-4,\n",
    "    'regularization': 0.,\n",
    "    'verbose': True,\n",
    "    'loss': 'mse',\n",
    "    'batch_size': 10\n",
    "}\n",
    "\n",
    "# Instantiate model\n",
    "gnarl = Gnarl(X_train, y_train, **nn_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add first hidden layer\n",
    "gnarl.add_layer(10, activation='leaky_relu')\n",
    "\n",
    "# Add second hidden layer\n",
    "gnarl.add_layer(5, activation='leaky_relu')\n",
    "\n",
    "# Add output layer\n",
    "gnarl.add_layer(1, activation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect the layers\n",
    "Finally, before we can start training the model using our Boston housing price data, we need to connect and build the computational graph that does all of the heavy-lifting behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Connect the layers\n",
    "gnarl.connect_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resulting model object\n",
    "Once the nodes in the graph have been connected, the gnarl model will have the following parameters and methods (clipped out defaults for brevity):\n",
    "\n",
    "```\n",
    "['X',\n",
    " '_biases',\n",
    " '_init',\n",
    " '_input_layer',\n",
    " '_log_probs',\n",
    " '_probs',\n",
    " '_reset_graph',\n",
    " '_update_input',\n",
    " '_update_output',\n",
    " '_weights',\n",
    " 'activation',\n",
    " 'batch_size',\n",
    " 'connect_layers',\n",
    " 'fit',\n",
    " 'graph',\n",
    " 'add_layer',\n",
    " 'layers_list',\n",
    " 'learning_rate',\n",
    " 'loss',\n",
    " 'nodes',\n",
    " 'predict',\n",
    " 'trainables',\n",
    " 'verbose',\n",
    " 'y']\n",
    "```\n",
    "\n",
    "> **Note:** properties prepended with an underscore are used internally, and shouldn't be accessed from outside the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Gnarl supports [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) and will automatically shuffle the data behind the scenes to make sure that the batch data is drawn from a random sample of the training data. The `epochs` argument determines how many times the training procedure will sample new batch data to train the network on. The `fit()` method also specifies an optional boolean argument `fit_more_data` that makes it easy to continue training an existing model.\n",
    "\n",
    "> **Note:** If you've set the optional argument `verbose=True`, you'll be able to see the network being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Solver: sgd\n",
      "Total number of samples: 303\n",
      "Steps per epoch: 30\n",
      "================================================================================\n",
      "Epoch: 1, Loss: 231.921\n",
      "Epoch: 500, Loss: 7.7780\n",
      "================================================================================\n",
      "Finished training model.\n",
      "Final loss: 5.308\n"
     ]
    }
   ],
   "source": [
    "# Train the model by sampling from all training data\n",
    "gnarl.fit(X_train, y_train, solver='sgd', epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict outputs\n",
    "Finally, once you've trained the model, and validated the hyperparameters using the validation data set `X_valid` and `y_valid` (not done here), we can predict Boston house prices and compute an $R^2$ score to see how well our model captures the variation in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared score is: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Predict outputs\n",
    "y_pred = gnarl.predict(X_test)\n",
    "\n",
    "# Compute R^2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the score\n",
    "print('The R-squared score is: %.2f' % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Gnarl supports multinomial classification using one-hot encoded labels. Any target class data must be transformed from a one-dimensional vector with unique class or target labels into a one-hot encoded matrix with dimensions `M x K` where `M` is the number of rows (training examples) and `K` is the number of class or target labels.\n",
    "\n",
    "Predictions during multi-class classification using Gnarl can return two types of results. First, predictions can be returned as a matrix of probabilities with the same dimensions as the test label data. Or, you can return the predictions as a one-dimensional vector of predicted labels (from `0` and counting upwards as integers). The latter is returned by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label vector:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "Dimension of label vector:\n",
      " (150,)\n"
     ]
    }
   ],
   "source": [
    "# Import one-hot encoder from sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Let's import the Iris data from sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "# Original labels contained in a one-dimensional vector\n",
    "X_iris = data.data\n",
    "y_iris = data.target\n",
    "print('Original label vector:\\n', y_iris)\n",
    "print('Dimension of label vector:\\n', y_iris.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `OneHotEncoder`, we can transform the `y_labels` vector into the appropriate form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M by K one-hot encoded label matrix:\n",
      " [[0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]]\n",
      "Dimension of one-hot encoded label matrix:\n",
      " (120, 3)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an encoder\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "# Fit and transform using the raw label vector y_labels\n",
    "y_ohe_iris = enc.fit_transform(y_iris.reshape(-1,1)).toarray().astype(int)\n",
    "\n",
    "# Split data - skipping validation split\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X_iris, y_ohe_iris, \n",
    "                                                                        test_size=0.2)\n",
    "\n",
    "# Print to confirm. Only first five samples shown for brevity.\n",
    "print('M by K one-hot encoded label matrix:\\n', y_train_iris[:5])\n",
    "print('Dimension of one-hot encoded label matrix:\\n', y_train_iris.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `OneHotEncoder` returns a sparse matrix, so we first transform the new matrix into a dense representation, and finally perform a type cast to `int` as a safety precaution. This last part is not strictly necessary, but it is more intuitive to think of the target labels being represented as integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict class labels\n",
    "During multi-class classification, Gnarl provides an optional boolean argument `truncate_labels` that finds which labels have the highest estimated probability per test input data, and returns these as a one-dimensional vector. By default, this parameter is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Solver: sgd\n",
      "Total number of samples: 120\n",
      "Steps per epoch: 24\n",
      "================================================================================\n",
      "Epoch: 1, Loss: 22.513\n",
      "Epoch: 500, Loss: 0.798\n",
      "================================================================================\n",
      "Finished training model.\n",
      "Final loss: 0.401\n"
     ]
    }
   ],
   "source": [
    "# Create simple model - using all default options\n",
    "gnarl_iris = Gnarl(X_train_iris, y_train_iris, \n",
    "                   loss='cross_entropy', \n",
    "                   batch_size=5, \n",
    "                   verbose=True)\n",
    "\n",
    "# Add a hidden layer\n",
    "gnarl_iris.add_layer(8, activation='leaky_relu')\n",
    "\n",
    "# Add output layer - 3 output classes\n",
    "gnarl_iris.add_layer(3, activation='none')\n",
    "\n",
    "# Connect layers\n",
    "gnarl_iris.connect_layers()\n",
    "\n",
    "# Train model\n",
    "gnarl_iris.fit(X_train_iris, y_train_iris, solver='sgd', epochs=500)\n",
    "\n",
    "# Predict\n",
    "y_pred_iris = gnarl_iris.predict(X_test_iris, truncate_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted class labels, when using the argument `truncate_labels=True` yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 1 2 2 1 2 1 0 2 0 1 0 0 2 0 2 0 1 2 0 1 0 2 2 2 2 2 1 0]\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_iris)\n",
    "print(y_pred_iris.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While, if we instead don't truncate the predictions, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.30815451e-04   1.45221431e-01   8.54647754e-01]\n",
      " [  1.27709228e-03   3.16332847e-01   6.82390061e-01]\n",
      " [  7.35623949e-02   7.95090687e-01   1.31346918e-01]\n",
      " [  1.97198720e-05   7.20170748e-02   9.27963205e-01]\n",
      " [  2.08362051e-03   3.42417838e-01   6.55498542e-01]]\n",
      "(30, 3)\n"
     ]
    }
   ],
   "source": [
    "# predict again\n",
    "y_pred_iris_probs = gnarl_iris.predict(X_test_iris, truncate_labels=False)\n",
    "\n",
    "print(y_pred_iris_probs[:5])\n",
    "print(y_pred_iris_probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also double-check that the probabilities sum to one, just to be safe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_pred_iris_probs, axis=1, keepdims=True)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can also quickly compute the accuracy score for the classification network to see how well the network performs (without any validation or tuning of hyperparameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 93.33 percent\n"
     ]
    }
   ],
   "source": [
    "def accuracy_score(y, y_pred):\n",
    "    return np.mean(y == y_pred) * 100\n",
    "\n",
    "# Turn test labels into original one-dim vector\n",
    "y_test_iris_trunc = np.argmax(y_test_iris, axis=1)\n",
    "\n",
    "print('Accuracy is: %.2f percent' % accuracy_score(y_test_iris_trunc, y_pred_iris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options\n",
    "Gnarl has the following options that can be used to set the hyperparameters of a neural network model:\n",
    "\n",
    "  * `activation: string` - Default is `'leaky_relu'`\n",
    "    \n",
    "    * `'sigmoid'`\n",
    "    \n",
    "    * `'relu'`\n",
    "    \n",
    "    * `'leaky_relu'`\n",
    "    \n",
    "    * `'none'` - Used to define output layer\n",
    "   \n",
    "  * `learning_rate: float` - Default is `1e-4`\n",
    "  * `regularization: float` - Default is `0.`\n",
    "  * `verbose: boolean` - Default is `False`\n",
    "  * `loss: string` - Default is `'mse'`\n",
    "    * `'mse'` - Mean Squared Error\n",
    "    * `'cross_entropy'` - Cross-entropy error (loss). Uses multinomial log loss.\n",
    "  * `batch_size: int` - Default is `10`, used during Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading models\n",
    "Gnarl also allows you to save and load your models for future use. The optional argument `path` allows you to specify where on disk to save the model. The default location is the same as the current directory. Pickle is the only file format currently supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Gnarl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-548b85307e14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import save method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mGnarl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgnarl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gnarls_barkley'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Gnarl'"
     ]
    }
   ],
   "source": [
    "# Import save method\n",
    "from Gnarl import save_model\n",
    "\n",
    "# Save model\n",
    "save_model(gnarl, model_name='gnarls_barkley', path='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a model, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import load method\n",
    "from Gnarl import load_model\n",
    "\n",
    "# Load model\n",
    "gnarls_barkley = load_model('./gnarls_barkley.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author\n",
    "_**Author:** Hugo Nordell_\n",
    "\n",
    "_**Twitter:** @hugonordell_\n",
    "\n",
    "_**License:** MIT_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 Machine Learning",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
